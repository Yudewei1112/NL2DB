from typing import List, Dict, Tuple, Any, Optional, TypedDict
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.runnables import RunnableLambda
from langchain_core.documents import Document
from langchain_community.document_loaders import UnstructuredExcelLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from FlagEmbedding import FlagReranker
import pandas as pd
import sqlite3
import os
import asyncio
import json
import hashlib
import time
import threading
from typing import List


# --- æ­¥éª¤ 0: å®šä¹‰å…¨å±€é…ç½® ---
VECTOR_DB_PATH = "vector_db.faiss"
VECTOR_DB_METADATA_PATH = "vector_db.pkl"
EXCEL_DIR = "uploads"  # å­˜æ”¾ Excel æ–‡ä»¶çš„ç›®å½•
EMBEDDING_MODEL_NAME = "moka-ai/m3e-base"
CACHE_DIR = "cache"
HEADER_CACHE_DIR = os.path.join(CACHE_DIR, "headers")

# åˆ›å»ºç¼“å­˜ç›®å½•
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(HEADER_CACHE_DIR, exist_ok=True)

# --- å•ä¾‹æ¨¡å¼çš„æ¨¡å‹ç®¡ç†å™¨ ---
class ModelManager:
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self._llm = None
            self._embedding_model = None
            self._reranker = None
            self._llm_config = {
                "provider": os.getenv("LLM_PROVIDER", "glm"),
                "model_name": os.getenv("LLM_MODEL_NAME", "glm-4-plus"),
                "temperature": float(os.getenv("LLM_TEMPERATURE", 0.2))
            }
            self._initialized = True
    
    def get_llm(self):
        """æ‡’åŠ è½½LLMæ¨¡å‹"""
        if self._llm is None:
            self._llm = self._create_llm(self._llm_config)
        return self._llm
    
    def get_embedding_model(self):
        """æ‡’åŠ è½½åµŒå…¥æ¨¡å‹"""
        if self._embedding_model is None:
            self._embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
        return self._embedding_model
    
    def get_reranker(self):
        """æ‡’åŠ è½½é‡æ’åºæ¨¡å‹"""
        if self._reranker is None:
            self._reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)
        return self._reranker
    
    def _create_llm(self, config):
        """åˆ›å»ºLLMå®ä¾‹"""
        provider = config.get("provider", "glm")
        if provider == "openai":
            from langchain_openai import ChatOpenAI
            return ChatOpenAI(
                openai_api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_BASE_URL"),
                model=config.get("model_name", "gpt-3.5-turbo"),
                temperature=config.get("temperature", 0.2)
            )
        elif provider == "glm":
            from langchain_community.chat_models.zhipuai import ChatZhipuAI
            return ChatZhipuAI(
                zhipuai_api_key=os.getenv("GLM_4_PLUS_API_KEY"),
                base_url=os.getenv("GLM_4_PLUS_API_BASE"),
                model=config.get("model_name", "glm-4-plus"),
                temperature=config.get("temperature", 0.2)
            )
        elif provider == "qwen":
            from langchain_community.chat_models.tongyi import ChatTongyi
            return ChatTongyi(
                dashscope_api_key=os.getenv("QWEN_API_KEY"),
                base_url=os.getenv("QWEN_API_BASE"),
                model_name=config.get("model_name", "qwen-turbo"),
                temperature=config.get("temperature", 0.2)
            )
        elif provider == "deepseek":
            from langchain_openai import ChatOpenAI
            return ChatOpenAI(
                openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
                base_url=os.getenv("DEEPSEEK_API_BASE"),
                model=config.get("model_name", "deepseek-coder"),
                temperature=config.get("temperature", 0.2)
            )
        elif provider == "claude":
            from langchain_anthropic import ChatAnthropic
            return ChatAnthropic(
                anthropic_api_key=os.getenv("CLAUDE_API_KEY"),
                base_url=os.getenv("CLAUDE_API_BASE"),
                model=config.get("model_name", "claude-3-7-sonnet"),
                temperature=config.get("temperature", 0.2)
            )
        else:
            print("Using mock LLM.")
            async def mock_llm(messages, config=None):
                return "SQL Query Placeholder"
            return RunnableLambda(mock_llm)

# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ModelManager()

# ============================================================================
# --- æ•°æ®å‡†å¤‡éƒ¨åˆ† ---
# ============================================================================

# --- å¯¼å…¥æ–°çš„æ•°æ®åº“ç®¡ç†å™¨ ---
from database_manager import get_database_manager

def get_table_mapping(excel_path: str) -> Dict[str, str]:
    """
    è·å–æŒ‡å®šExcelæ–‡ä»¶çš„è¡¨æ˜ å°„
    
    Args:
        excel_path: Excelæ–‡ä»¶è·¯å¾„
        
    Returns:
        è¡¨æ˜ å°„å­—å…¸ {sheet_name: table_name}
    """
    return get_database_manager().get_table_mapping(excel_path)

def get_enhanced_table_mapping(excel_name: str = None) -> Dict[Tuple[str, str], str]:
    """
    è·å–å¢å¼ºçš„è¡¨æ˜ å°„ï¼ˆæ–¹æ¡ˆ1ï¼šåŒ…å«excel_nameçš„æ˜ å°„ç»“æ„ï¼‰
    
    Args:
        excel_name: Excelæ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›æ‰€æœ‰æ˜ å°„
        
    Returns:
        å¢å¼ºæ˜ å°„å­—å…¸ {(excel_name, sheet_name): table_name}
    """
    return get_database_manager().get_enhanced_table_mapping(excel_name)

def get_table_name_by_excel_sheet(excel_name: str, sheet_name: str) -> str:
    """
    æ ¹æ®Excelæ–‡ä»¶åå’ŒSheetåè·å–å¯¹åº”çš„è¡¨åï¼ˆæ–¹æ¡ˆ1ä¸“ç”¨æ–¹æ³•ï¼‰
    
    Args:
        excel_name: Excelæ–‡ä»¶å
        sheet_name: Sheetåç§°
        
    Returns:
        å¯¹åº”çš„æ•°æ®åº“è¡¨åï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
    """
    return get_database_manager().get_table_name_by_excel_sheet(excel_name, sheet_name)

# --- è¡¨å¤´ä¿¡æ¯ç¼“å­˜ç®¡ç†å™¨ ---
class HeaderCacheManager:
    def __init__(self, cache_dir: str = HEADER_CACHE_DIR):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        self.memory_cache = {}  # å†…å­˜ç¼“å­˜
        self.max_memory_cache = 100  # æœ€å¤§å†…å­˜ç¼“å­˜æ•°é‡
    
    def get_cache_key(self, excel_path: str, sheet_name: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        excel_name = os.path.splitext(os.path.basename(excel_path))[0]
        return f"{excel_name}_{sheet_name}"
    
    def get_cache_path(self, excel_path: str, sheet_name: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_key = self.get_cache_key(excel_path, sheet_name)
        return os.path.join(self.cache_dir, f"{cache_key}.json")
    
    def get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def load_cached_header(self, excel_path: str, sheet_name: str) -> Optional[str]:
        """åŠ è½½ç¼“å­˜çš„è¡¨å¤´ä¿¡æ¯"""
        cache_key = self.get_cache_key(excel_path, sheet_name)
        
        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self.memory_cache:
            cache_data = self.memory_cache[cache_key]
            if self._is_cache_valid(excel_path, cache_data):
                return cache_data['header_info']
            else:
                del self.memory_cache[cache_key]
        
        # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
        cache_path = self.get_cache_path(excel_path, sheet_name)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                if self._is_cache_valid(excel_path, cache_data):
                    # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                    self._add_to_memory_cache(cache_key, cache_data)
                    return cache_data['header_info']
                else:
                    os.remove(cache_path)
            except Exception:
                if os.path.exists(cache_path):
                    os.remove(cache_path)
        
        return None
    
    def cache_header_analysis(self, excel_path: str, sheet_name: str, header_info: str):
        """ç¼“å­˜è¡¨å¤´åˆ†æç»“æœ"""
        cache_data = {
            'excel_path': excel_path,
            'sheet_name': sheet_name,
            'header_info': header_info,
            'timestamp': time.time(),
            'file_hash': self.get_file_hash(excel_path),
            'model_version': 'v1.0'
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶ç¼“å­˜
        cache_path = self.get_cache_path(excel_path, sheet_name)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
        cache_key = self.get_cache_key(excel_path, sheet_name)
        self._add_to_memory_cache(cache_key, cache_data)
    
    def _is_cache_valid(self, excel_path: str, cache_data: dict) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(excel_path):
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å“ˆå¸Œæ˜¯å¦åŒ¹é…
            current_hash = self.get_file_hash(excel_path)
            if current_hash != cache_data.get('file_hash'):
                return False
            
            # æ£€æŸ¥æ¨¡å‹ç‰ˆæœ¬
            if cache_data.get('model_version') != 'v1.0':
                return False
            
            return True
        except Exception:
            return False
    
    def _add_to_memory_cache(self, cache_key: str, cache_data: dict):
        """æ·»åŠ åˆ°å†…å­˜ç¼“å­˜"""
        if len(self.memory_cache) >= self.max_memory_cache:
            # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = min(self.memory_cache.keys(), 
                           key=lambda k: self.memory_cache[k]['timestamp'])
            del self.memory_cache[oldest_key]
        
        self.memory_cache[cache_key] = cache_data
    
    def clear_cache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        self.memory_cache.clear()
        for filename in os.listdir(self.cache_dir):
            file_path = os.path.join(self.cache_dir, filename)
            if os.path.isfile(file_path):
                os.remove(file_path)
    
    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        file_cache_count = len([f for f in os.listdir(self.cache_dir) if f.endswith('.json')])
        return {
            'memory_cache_count': len(self.memory_cache),
            'file_cache_count': file_cache_count,
            'max_memory_cache': self.max_memory_cache
        }

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
header_cache_manager = HeaderCacheManager()



# --- ä¼˜åŒ–åçš„è¾…åŠ©å‡½æ•° ---
def excel_to_sqlite(excel_path: str, db_path: str = "database.db") -> Tuple[str, Dict[str, str]]:
    """å°† Excel æ–‡ä»¶è½¬æ¢ä¸º SQLite æ•°æ®åº“"""
    db_manager = get_database_manager()
    db_manager.update_if_changed(excel_path)
    return db_manager.db_path, db_manager.get_table_mapping(excel_path)

async def identify_header_with_cache(excel_path: str, sheet_name: str, llm_model) -> Optional[str]:
    """å¸¦ç¼“å­˜çš„è¡¨å¤´è¯†åˆ«"""
    # å°è¯•ä»ç¼“å­˜åŠ è½½
    cached_header = header_cache_manager.load_cached_header(excel_path, sheet_name)
    if cached_header:
        return cached_header
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡ŒLLMåˆ†æ
    header_info = await identify_header(excel_path, sheet_name, llm_model)
    
    # ç¼“å­˜ç»“æœ
    if header_info:
        header_cache_manager.cache_header_analysis(excel_path, sheet_name, str(header_info))
    
    return header_info

async def identify_header(excel_path: str, sheet_name: str, llm_model):
    """ä½¿ç”¨å¤§æ¨¡å‹è¯†åˆ« Excel Sheet çš„è¡¨å¤´å’Œå…³é”®ä¿¡æ¯"""
    try:
        df = pd.read_excel(excel_path, sheet_name=sheet_name)
        
        content_lines = []
        headers = df.columns.tolist()
        content_lines.append("è¡¨å¤´: " + " | ".join(str(h) for h in headers))
        
        content_lines.append("\næ•°æ®æ ·æœ¬:")
        for idx, row in df.iterrows():
            row_data = " | ".join(str(val) for val in row.values)
            content_lines.append(f"ç¬¬{idx+1}è¡Œ: {row_data}")
        
        content_lines.append(f"\næ•°æ®ç»Ÿè®¡: å…±{len(df)}è¡Œï¼Œ{len(df.columns)}åˆ—")
        content = "\n".join(content_lines)
        
        with open("excel_header_prompt.txt", "r", encoding="utf-8") as f:
            HEADER_PROMPT = f.read()
        prompt = f"{HEADER_PROMPT}\n\nè¯·åˆ†æä»¥ä¸‹ Excel è¡¨æ ¼ç‰‡æ®µï¼Œå¹¶è¯†åˆ«å‡ºè¡¨æ ¼åç§°ã€è¡¨å¤´å’Œå…³é”®ä¿¡æ¯ï¼š\n---\n{content}\n---\nè¡¨å¤´å’Œå…³é”®ä¿¡æ¯æ˜¯: "
        
        messages = [HumanMessage(content=prompt)]
        response = await llm_model.ainvoke(messages)
        
        # ä¿®å¤AIMessageå¯¹è±¡å¤„ç† - æå–contentå±æ€§
        if hasattr(response, 'content'):
            return response.content
        else:
            return str(response)
    except Exception as e:
        return None

# å¹¶å‘å¤„ç†è¡¨å¤´è¯†åˆ«
async def identify_headers_concurrently(excel_path: str, sheet_names: List[str], llm_model, max_workers: int = 3):
    """å¹¶å‘å¤„ç†å¤šä¸ªè¡¨å¤´è¯†åˆ«"""
    results = {}
    
    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(max_workers)
    
    async def process_sheet(sheet_name):
        async with semaphore:
            return sheet_name, await identify_header_with_cache(excel_path, sheet_name, llm_model)
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = [process_sheet(sheet_name) for sheet_name in sheet_names]
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)
    
    # å¤„ç†ç»“æœ
    for result in completed_tasks:
        if isinstance(result, Exception):
            continue
        sheet_name, header = result
        if header:
            results[sheet_name] = header
    
    return results

def get_file_modification_time(file_path: str) -> float:
    """è·å–æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´æˆ³"""
    try:
        return os.path.getmtime(file_path)
    except OSError:
        return 0.0

def save_vector_db_metadata(vector_db_dir: str, excel_files_info: Dict[str, float]):
    """ä¿å­˜å‘é‡æ•°æ®åº“çš„å…ƒæ•°æ®ä¿¡æ¯"""
    metadata_path = os.path.join(vector_db_dir, "vector_db_metadata.json")
    metadata = {
        "last_update": time.time(),
        "excel_files": excel_files_info
    }
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

def load_vector_db_metadata(vector_db_dir: str) -> Dict[str, Any]:
    """åŠ è½½å‘é‡æ•°æ®åº“çš„å…ƒæ•°æ®ä¿¡æ¯"""
    metadata_path = os.path.join(vector_db_dir, "vector_db_metadata.json")
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def check_excel_files_changes(excel_dir: str, existing_metadata: Dict[str, Any]) -> Tuple[bool, Dict[str, float]]:
    """æ£€æŸ¥Excelæ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–ï¼ˆæ–°å¢ã€ä¿®æ”¹æˆ–åˆ é™¤ï¼‰"""
    current_files = {}
    
    # è·å–å½“å‰æ‰€æœ‰Excelæ–‡ä»¶çš„ä¿¡æ¯
    if os.path.exists(excel_dir):
        for filename in os.listdir(excel_dir):
            if filename.endswith(('.xlsx', '.xls')):
                file_path = os.path.join(excel_dir, filename)
                current_files[filename] = get_file_modification_time(file_path)
    
    # è·å–ä¹‹å‰è®°å½•çš„æ–‡ä»¶ä¿¡æ¯
    previous_files = existing_metadata.get('excel_files', {})
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
    has_changes = False
    
    # æ£€æŸ¥æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶
    for filename, mod_time in current_files.items():
        if filename not in previous_files or previous_files[filename] != mod_time:
            has_changes = True
            print(f"æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–: {filename}")
            break
    
    # æ£€æŸ¥åˆ é™¤çš„æ–‡ä»¶
    if not has_changes:
        for filename in previous_files:
            if filename not in current_files:
                has_changes = True
                print(f"æ£€æµ‹åˆ°æ–‡ä»¶åˆ é™¤: {filename}")
                break
    
    return has_changes, current_files

async def create_and_store_vectors(excel_dir: str, llm_model, embedding_model, force_recreate: bool = False):
    """åˆ›å»ºå’Œå­˜å‚¨å‘é‡æ•°æ®åº“ï¼ˆé›†æˆäº†åŠ è½½å’Œåˆ›å»ºåŠŸèƒ½ï¼Œæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
    # ä¿®æ”¹å…¨å±€é…ç½®
    VECTOR_DB_DIR = "Faiss"
    FAISS_INDEX_PATH = os.path.join(VECTOR_DB_DIR, "faiss_index.faiss")
    FAISS_INDEX_PKL_PATH = os.path.join(VECTOR_DB_DIR, "faiss_index.pkl")
    
    os.makedirs(VECTOR_DB_DIR, exist_ok=True)
    
    # åŠ è½½ç°æœ‰çš„å…ƒæ•°æ®
    existing_metadata = load_vector_db_metadata(VECTOR_DB_DIR)
    
    # æ£€æŸ¥Excelæ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
    has_changes, current_files_info = check_excel_files_changes(excel_dir, existing_metadata)
    
    # å¦‚æœä¸å¼ºåˆ¶é‡æ–°åˆ›å»ºä¸”å‘é‡æ•°æ®åº“å­˜åœ¨ä¸”æ²¡æœ‰æ–‡ä»¶å˜åŒ–ï¼Œåˆ™ç›´æ¥åŠ è½½
    if (not force_recreate and 
        os.path.exists(FAISS_INDEX_PATH) and 
        os.path.exists(FAISS_INDEX_PKL_PATH) and 
        not has_changes):
        try:
            print("å‘é‡æ•°æ®åº“å·²å­˜åœ¨ä¸”Excelæ–‡ä»¶æ— å˜åŒ–ï¼Œç›´æ¥åŠ è½½ç°æœ‰æ•°æ®åº“")
            return FAISS.load_local(VECTOR_DB_DIR, embedding_model, index_name="faiss_index", allow_dangerous_deserialization=True)
        except Exception as e:
            print(f"åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“å¤±è´¥: {e}ï¼Œå°†é‡æ–°åˆ›å»º")
            try:
                if os.path.exists(FAISS_INDEX_PATH):
                    os.remove(FAISS_INDEX_PATH)
                if os.path.exists(FAISS_INDEX_PKL_PATH):
                    os.remove(FAISS_INDEX_PKL_PATH)
            except Exception:
                pass
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ›å»ºå‘é‡æ•°æ®åº“
    need_recreate = (
        has_changes or 
        force_recreate or 
        not os.path.exists(FAISS_INDEX_PATH) or 
        not os.path.exists(FAISS_INDEX_PKL_PATH)
    )
    
    if need_recreate:
        if has_changes:
            print("æ£€æµ‹åˆ°Excelæ–‡ä»¶å˜åŒ–ï¼Œæ­£åœ¨æ›´æ–°å‘é‡æ•°æ®åº“...")
        elif force_recreate:
            print("å¼ºåˆ¶é‡æ–°åˆ›å»ºå‘é‡æ•°æ®åº“...")
        else:
            print("å‘é‡æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
    else:
        # å¦‚æœä¸éœ€è¦é‡æ–°åˆ›å»ºï¼Œç›´æ¥è¿”å›Noneï¼ˆè¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºå‰é¢å·²ç»å¤„ç†äº†åŠ è½½é€»è¾‘ï¼‰
        print("å‘é‡æ•°æ®åº“æ— éœ€é‡æ–°åˆ›å»º")
        return None
    
    # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
    all_documents = []
    
    # å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰Excelæ–‡ä»¶
    for filename in os.listdir(excel_dir):
        if filename.endswith(('.xlsx', '.xls')):
            excel_path = os.path.join(excel_dir, filename)
            print(f"å¤„ç†Excelæ–‡ä»¶: {excel_path}")
            
            try:
                excel_file = pd.ExcelFile(excel_path)
                sheet_names = excel_file.sheet_names
                
                # å¹¶å‘å¤„ç†è¡¨å¤´è¯†åˆ«
                headers_results = await identify_headers_concurrently(excel_path, sheet_names, llm_model)
                
                for sheet_name in sheet_names:
                    header = headers_results.get(sheet_name)
                    if header:
                        sheet_header_mapping = f"Sheetåç§°: {sheet_name}, è¡¨å¤´: {header}"
                        text_to_embed = f"{os.path.basename(excel_path)}-{sheet_header_mapping}"
                        
                        doc = Document(
                            page_content=text_to_embed,
                            metadata={
                                "excel_name": os.path.basename(excel_path),
                                "sheet_name": sheet_name,
                                "header": header,
                                "mapping_text": sheet_header_mapping
                            }
                        )
                        all_documents.append(doc)
                        
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {excel_path} æ—¶å‡ºé”™: {e}")
                continue
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“
    if all_documents:
        vectorstore = FAISS.from_documents(all_documents, embedding_model)
        vectorstore.save_local(VECTOR_DB_DIR, index_name="faiss_index")
        print(f"æˆåŠŸåˆ›å»ºå‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {len(all_documents)} ä¸ªæ–‡æ¡£")
        
        # ä¿å­˜å…ƒæ•°æ®ä¿¡æ¯
        save_vector_db_metadata(VECTOR_DB_DIR, current_files_info)
        print("å·²ä¿å­˜å‘é‡æ•°æ®åº“å…ƒæ•°æ®ä¿¡æ¯")
        
        return vectorstore
    else:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£ï¼Œåˆ›å»ºç©ºçš„å‘é‡æ•°æ®åº“")
        dummy_doc = Document(page_content="dummy", metadata={})
        vectorstore = FAISS.from_documents([dummy_doc], embedding_model)
        vectorstore.save_local(VECTOR_DB_DIR, index_name="faiss_index")
        
        # å³ä½¿æ˜¯ç©ºæ•°æ®åº“ä¹Ÿè¦ä¿å­˜å…ƒæ•°æ®
        save_vector_db_metadata(VECTOR_DB_DIR, current_files_info)
        
        return vectorstore
# ============================================================================
# --- LangGraph å·¥ä½œæµéƒ¨åˆ† ---
# ============================================================================

from langgraph.graph import StateGraph, END
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage

class GraphState(TypedDict):
    """LangGraphå·¥ä½œæµçŠ¶æ€å®šä¹‰"""
    query: str
    excel_path: str
    db_path: str
    table_mapping: Dict[str, str]
    vectorstore: FAISS
    relevant_sheets: List[Tuple[str, str]]
    reranked_sheets: List[Tuple[str, str]]
    sql_query: str
    db_results: Any
    response: str

def get_relevant_sheets(state: GraphState):
    """ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„Excel Sheets"""
    query = state['query']
    vectorstore = state['vectorstore']
    
    results = vectorstore.similarity_search_with_score(query, k=5)
    
    print(f"\nğŸ” [SIMILARITY DEBUG] å‘é‡æ£€ç´¢ç»“æœ (æŸ¥è¯¢: {query})")
    relevant_sheets = []
    seen_sheets = set()  # ç”¨äºå»é‡
    
    for i, (doc, score) in enumerate(results):
        excel_name = doc.metadata.get('excel_name', '')
        sheet_name = doc.metadata.get('sheet_name', '')
        header_info = doc.metadata.get('header', '')
        
        print(f"  ç¬¬{i+1}å: Excel={excel_name}, Sheet={sheet_name}, ç›¸ä¼¼åº¦={score:.4f}")
        print(f"         è¡¨å¤´ä¿¡æ¯: {header_info[:100]}..." if len(header_info) > 100 else f"         è¡¨å¤´ä¿¡æ¯: {header_info}")
        
        if excel_name and sheet_name:
            # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦è¿›è¡Œå»é‡
            sheet_key = (excel_name, sheet_name)
            if sheet_key not in seen_sheets:
                relevant_sheets.append((excel_name, sheet_name))
                seen_sheets.add(sheet_key)
                print(f"         âœ… å·²æ·»åŠ åˆ°å€™é€‰åˆ—è¡¨")
            else:
                print(f"         âš ï¸ é‡å¤sheetï¼Œå·²è·³è¿‡")
    
    print(f"\nğŸ“‹ [SIMILARITY DEBUG] æœ€ç»ˆå€™é€‰sheetsæ•°é‡: {len(relevant_sheets)}")
    for i, (excel_name, sheet_name) in enumerate(relevant_sheets):
        print(f"  å€™é€‰{i+1}: {excel_name} - {sheet_name}")
    
    return {"relevant_sheets": relevant_sheets}

def rerank_sheets(state: GraphState):
    """ä½¿ç”¨ rerank æ¨¡å‹å¯¹å¬å›çš„ Excel Sheets è¿›è¡Œé‡æ’åº"""
    query = state['query']
    reranker = model_manager.get_reranker()
    
    print(f"\nğŸ”„ [RERANK DEBUG] å¼€å§‹é‡æ’åº (å€™é€‰æ•°é‡: {len(state['relevant_sheets'])})")
    
    if len(state['relevant_sheets']) <= 2:
        print(f"ğŸ“ [RERANK DEBUG] å€™é€‰æ•°é‡â‰¤2ï¼Œè·³è¿‡é‡æ’åº")
        state['reranked_sheets'] = state['relevant_sheets']
    else:
        pairs = []
        
        print(f"ğŸ” [RERANK DEBUG] æ„å»ºé‡æ’åºå¯¹æ¯”æ–‡æœ¬:")
        for i, (excel_name, sheet_name) in enumerate(state['relevant_sheets']):
            results = state['vectorstore'].similarity_search(
                f"{excel_name}-{sheet_name}", k=1
            )
            if results:
                header_info = results[0].metadata.get('mapping_text', '')
                pairs.append((query, header_info))
                print(f"  å¯¹æ¯”{i+1}: {excel_name}-{sheet_name}")
                print(f"         æ˜ å°„æ–‡æœ¬: {header_info}")
            else:
                pairs.append((query, f"{excel_name}-{sheet_name}"))
                print(f"  å¯¹æ¯”{i+1}: {excel_name}-{sheet_name} (æœªæ‰¾åˆ°æ˜ å°„æ–‡æœ¬)")
        
        print(f"\nğŸ§® [RERANK DEBUG] è®¡ç®—é‡æ’åºåˆ†æ•°...")
        scores = reranker.compute_score(pairs)
        
        print(f"ğŸ“Š [RERANK DEBUG] é‡æ’åºåˆ†æ•°ç»“æœ:")
        for i, ((excel_name, sheet_name), score) in enumerate(zip(state['relevant_sheets'], scores)):
            print(f"  ç¬¬{i+1}å: {excel_name}-{sheet_name}, é‡æ’åºåˆ†æ•°={score:.4f}")
        
        ranked_results = sorted(
            zip(state['relevant_sheets'], scores), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print(f"\nğŸ† [RERANK DEBUG] é‡æ’åºåçš„æœ€ç»ˆæ’å:")
        for i, ((excel_name, sheet_name), score) in enumerate(ranked_results):
            print(f"  æ’å{i+1}: {excel_name}-{sheet_name}, åˆ†æ•°={score:.4f}")
        
        state['reranked_sheets'] = [item[0] for item in ranked_results[:3]]
        
        print(f"\nâœ… [RERANK DEBUG] æœ€ç»ˆé€‰æ‹©çš„å‰3åsheets:")
        for i, (excel_name, sheet_name) in enumerate(state['reranked_sheets']):
            print(f"  é€‰æ‹©{i+1}: {excel_name}-{sheet_name}")
    
    return {"reranked_sheets": state['reranked_sheets']}

async def generate_sql(state: GraphState):
    """æ ¹æ®é‡æ’åºçš„sheetså’Œç”¨æˆ·é—®é¢˜ç”ŸæˆSQLæŸ¥è¯¢ï¼ˆæ”¯æŒæ–¹æ¡ˆ1å’Œæ–¹æ¡ˆ2ï¼ŒåŒ…å«åˆ—åä¸šåŠ¡å«ä¹‰æ˜ å°„ï¼‰"""
    query = state['query']
    reranked_sheets = state['reranked_sheets']
    llm = model_manager.get_llm()
    
    # å¯¼å…¥database_managerå¹¶è·å–å®ä¾‹
    from database_manager import DatabaseManager
    db_manager = DatabaseManager()
    
    schema_info = []
    table_names = []
    column_mappings_text = ""
    
    # æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨å¢å¼ºæ˜ å°„ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
    try:
        enhanced_mapping = db_manager.get_enhanced_table_mapping()
        print(f"\nğŸ”§ [SQL DEBUG] è·å–åˆ°å¢å¼ºæ˜ å°„: {len(enhanced_mapping)} æ¡è®°å½•")
    except AttributeError:
        enhanced_mapping = {}
        print(f"\nğŸ”§ [SQL DEBUG] å¢å¼ºæ˜ å°„æ–¹æ³•ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ–¹æ¡ˆ2")
    
    # è¯»å–æ˜ å°„æ³¨å†Œè¡¨
    mapping_registry_path = os.path.join("column_mapping_docs", "mapping_registry.json")
    mapping_registry = {}
    try:
        with open(mapping_registry_path, 'r', encoding='utf-8') as f:
            mapping_registry = json.load(f)
        print(f"ğŸ“‹ [SQL DEBUG] æˆåŠŸåŠ è½½æ˜ å°„æ³¨å†Œè¡¨ï¼ŒåŒ…å« {len(mapping_registry)} ä¸ªè¡¨çš„æ˜ å°„é…ç½®")
    except Exception as e:
        print(f"âš ï¸ [SQL DEBUG] åŠ è½½æ˜ å°„æ³¨å†Œè¡¨å¤±è´¥: {e}")
    
    # éå†é‡æ’åºçš„sheets
    for excel_name, sheet_name in reranked_sheets:
        print(f"\nğŸ” [SQL DEBUG] å¤„ç† Excel: {excel_name}, Sheet: {sheet_name}")
        
        table_name = None
        
        # æ–¹æ¡ˆ1ï¼šä¼˜å…ˆä½¿ç”¨å¢å¼ºæ˜ å°„
        if (excel_name, sheet_name) in enhanced_mapping:
            table_name = enhanced_mapping[(excel_name, sheet_name)]
            print(f"âœ… [SQL DEBUG] æ–¹æ¡ˆ1æˆåŠŸ: ({excel_name}, {sheet_name}) -> {table_name}")
        else:
            # æ–¹æ¡ˆ2ï¼šå›é€€åˆ°åŠ¨æ€è·å–æ˜ å°„
            print(f"ğŸ”„ [SQL DEBUG] æ–¹æ¡ˆ1æœªæ‰¾åˆ°ï¼Œä½¿ç”¨æ–¹æ¡ˆ2åŠ¨æ€è·å–")
            try:
                excel_path = os.path.join("uploads", excel_name)
                file_table_mapping = db_manager.get_table_mapping(excel_path)
                print(f"ğŸ“‹ [SQL DEBUG] åŠ¨æ€è·å–çš„è¡¨æ˜ å°„: {file_table_mapping}")
                
                if sheet_name in file_table_mapping:
                    table_name = file_table_mapping[sheet_name]
                    print(f"âœ… [SQL DEBUG] æ–¹æ¡ˆ2æˆåŠŸ: {sheet_name} -> {table_name}")
                else:
                    print(f"âŒ [SQL DEBUG] æ–¹æ¡ˆ2å¤±è´¥: {sheet_name} ä¸åœ¨ {list(file_table_mapping.keys())}")
            except Exception as e:
                print(f"âŒ [SQL DEBUG] æ–¹æ¡ˆ2å¼‚å¸¸: {e}")
        
        if table_name:
            table_names.append(table_name)
            
            try:
                conn = sqlite3.connect(db_manager.db_path)
                cursor = conn.cursor()
                cursor.execute(f"PRAGMA table_info({table_name})")
                columns = cursor.fetchall()
                conn.close()
                
                column_names = [col[1] for col in columns]
                column_names_display = ', '.join(column_names)
                schema_info.append(f"è¡¨å: {table_name} (æ¥æº: {excel_name}-{sheet_name}), åˆ—å: {column_names_display}")
                
                # è·å–åˆ—åä¸šåŠ¡å«ä¹‰æ˜ å°„
                if table_name in mapping_registry:
                    config_path = mapping_registry[table_name]['config_path']
                    full_config_path = os.path.join("column_mapping_docs", os.path.basename(config_path))
                    
                    try:
                        with open(full_config_path, 'r', encoding='utf-8') as f:
                            column_config = json.load(f)
                        
                        column_mappings = column_config.get('column_mappings', {})
                        if column_mappings:
                            column_mappings_text += f"\n\nè¡¨ {table_name} çš„åˆ—åä¸šåŠ¡å«ä¹‰æ˜ å°„:\n"
                            for db_col, business_meaning in column_mappings.items():
                                column_mappings_text += f"  - {db_col} â†’ {business_meaning}\n"
                            print(f"ğŸ“‹ [SQL DEBUG] æˆåŠŸåŠ è½½è¡¨ {table_name} çš„åˆ—åæ˜ å°„é…ç½®")
                        else:
                            print(f"âš ï¸ [SQL DEBUG] è¡¨ {table_name} çš„æ˜ å°„é…ç½®ä¸ºç©º")
                            
                    except Exception as e:
                        print(f"âš ï¸ [SQL DEBUG] åŠ è½½è¡¨ {table_name} çš„åˆ—åæ˜ å°„é…ç½®å¤±è´¥: {e}")
                else:
                    print(f"âš ï¸ [SQL DEBUG] æœªæ‰¾åˆ°è¡¨ {table_name} çš„åˆ—åæ˜ å°„é…ç½®")
                
                print(f"âœ… [SQLæ˜ å°„] æˆåŠŸæ˜ å°„: {excel_name}-{sheet_name} -> {table_name}")
            except Exception as e:
                print(f"âŒ [SQLæ˜ å°„] è·å–è¡¨ç»“æ„å¤±è´¥ {table_name}: {e}")
        else:
            print(f"âš ï¸ [SQLæ˜ å°„] æœªæ‰¾åˆ°æ˜ å°„: {excel_name}-{sheet_name}")
    
    schema_text = "\n".join(schema_info)
    
    # æ„å»ºå®Œæ•´çš„æ˜ å°„è¯´æ˜
    mapping_instruction = ""
    if column_mappings_text:
        mapping_instruction = f"\n\nåˆ—åä¸šåŠ¡å«ä¹‰æ˜ å°„:{column_mappings_text}"
    
    # æ„å»ºå¤šè¡¨æŸ¥è¯¢æŒ‡å¯¼
    multi_table_instruction = ""
    if len(table_names) > 1:
        table_list = "ã€".join(table_names)
        multi_table_instruction = f"\n\nğŸ” å¤šè¡¨æŸ¥è¯¢è¦æ±‚ï¼š\n- å½“å‰å¬å›äº† {len(table_names)} ä¸ªç›¸å…³è¡¨ï¼š{table_list}\n- å¿…é¡»ä»æ‰€æœ‰ç›¸å…³è¡¨ä¸­æŸ¥è¯¢æ•°æ®ï¼Œä½¿ç”¨ UNION ALL åˆå¹¶ç»“æœ\n- æ¯ä¸ªè¡¨éƒ½åº”è¯¥åŒ…å«ç›¸åŒçš„æŸ¥è¯¢æ¡ä»¶å’Œåˆ—é€‰æ‹©\n- ç¡®ä¿æŸ¥è¯¢è¦†ç›–æ‰€æœ‰å¯èƒ½åŒ…å«ç›®æ ‡æ•°æ®çš„è¡¨"
    
    sql_prompt = f"""æ ¹æ®ä»¥ä¸‹æ•°æ®åº“è¡¨ç»“æ„å’Œç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆç›¸åº”çš„SQLæŸ¥è¯¢è¯­å¥ã€‚

æ•°æ®åº“è¡¨ç»“æ„ï¼š
{schema_text}{mapping_instruction}

ç”¨æˆ·é—®é¢˜ï¼š{query}

é‡è¦æç¤ºï¼š
1. è¯·ä»”ç»†ç†è§£ç”¨æˆ·é—®é¢˜ä¸­æåˆ°çš„ä¸šåŠ¡æœ¯è¯­ï¼Œå¹¶æ ¹æ®åˆ—åæ˜ å°„æ‰¾åˆ°å¯¹åº”çš„æ•°æ®åº“åˆ—å
2. åœ¨WHEREæ¡ä»¶ä¸­ï¼Œå¯¹äºæ–‡æœ¬åŒ¹é…è¯·ä½¿ç”¨LIKEæ“ä½œç¬¦å’Œé€šé…ç¬¦%ï¼Œè€Œä¸æ˜¯ç²¾ç¡®åŒ¹é…ï¼ˆ=ï¼‰
3. ä¾‹å¦‚ï¼šå¦‚æœç”¨æˆ·é—®é¢˜æ¶‰åŠ"äº§å“åç§°"ç›¸å…³å†…å®¹ï¼Œåº”è¯¥ä½¿ç”¨å¯¹åº”çš„æ•°æ®åº“åˆ—åå¦‚ WHERE `Unnamed: 1` LIKE '%LEDçº³ç±³æ¨¡å—ç¯%'
4. åœ¨SQLæŸ¥è¯¢ä¸­å¿…é¡»ä½¿ç”¨æ•°æ®åº“åˆ—åï¼ˆå¦‚Unnamed: 1ï¼‰ï¼Œè€Œä¸æ˜¯ä¸šåŠ¡å«ä¹‰åç§°
5. æ ¹æ®ç”¨æˆ·é—®é¢˜çš„è¯­ä¹‰ï¼Œæ™ºèƒ½é€‰æ‹©éœ€è¦æŸ¥è¯¢çš„åˆ—å’Œè¿‡æ»¤æ¡ä»¶
6. å¦‚æœæä¾›äº†åˆ—åä¸šåŠ¡å«ä¹‰æ˜ å°„ï¼Œè¯·æ ¹æ®æ˜ å°„å…³ç³»å°†ç”¨æˆ·é—®é¢˜ä¸­çš„ä¸šåŠ¡æœ¯è¯­è½¬æ¢ä¸ºå¯¹åº”çš„æ•°æ®åº“åˆ—å
7. âš ï¸ å…³é”®è¦æ±‚ï¼šå¦‚æœç³»ç»Ÿå¬å›äº†å¤šä¸ªè¡¨ï¼Œè¯·ä¸ºæ¯ä¸ªè¡¨ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„SQLæŸ¥è¯¢è¯­å¥ã€‚
8. å•è¡¨æŸ¥è¯¢ç¤ºä¾‹æ ¼å¼ï¼š
   SELECT * FROM table1 WHERE condition;
   SELECT * FROM table2 WHERE condition;
   SELECT * FROM table3 WHERE condition;

è¯·ç”ŸæˆSQLæŸ¥è¯¢è¯­å¥ï¼ˆåªè¿”å›SQLè¯­å¥ï¼Œä¸è¦å…¶ä»–è§£é‡Šï¼‰ï¼š"""
    
    messages = [HumanMessage(content=sql_prompt)]
    response = await llm.ainvoke(messages)
    
    sql_query = str(response.content).strip()
    
    if sql_query.startswith('```sql'):
        sql_query = sql_query[6:]
    if sql_query.startswith('```'):
        sql_query = sql_query[3:]
    if sql_query.endswith('```'):
        sql_query = sql_query[:-3]
    
    sql_query = sql_query.strip()
    
    # è°ƒè¯•è¾“å‡ºï¼šç”Ÿæˆçš„SQLè¯­å¥
    print(f"\nğŸ”§ [SQL DEBUG] ç”Ÿæˆçš„SQLæŸ¥è¯¢è¯­å¥:")
    print(f"ğŸ“ [SQL DEBUG] {sql_query}")
    print(f"ğŸ¯ [SQL DEBUG] æŸ¥è¯¢ç›®æ ‡è¡¨: {', '.join(table_names)}")
    print(f"ğŸ”— [SQL DEBUG] åˆ—åæ˜ å°„ä¿¡æ¯: {column_mappings_text}")
    
    return {"sql_query": sql_query}

def execute_sql(state: GraphState):
    """æ‰§è¡Œç”Ÿæˆçš„SQLæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
    
    æ”¯æŒæ‰§è¡Œå¤šæ¡ç‹¬ç«‹çš„SQLè¯­å¥ï¼Œæ¯ä¸ªæŸ¥è¯¢ç»“æœä»¥JSONæ ¼å¼ç‹¬ç«‹è¾“å‡º
    """
    import json
    
    sql_query = state['sql_query']
    db_path = state['db_path']
    
    print(f"\nâš¡ [SQL DEBUG] å¼€å§‹æ‰§è¡ŒSQLæŸ¥è¯¢")
    print(f"ğŸ—„ï¸ [SQL DEBUG] æ•°æ®åº“è·¯å¾„: {db_path}")
    print(f"ğŸ“ [SQL DEBUG] æ‰§è¡Œçš„SQL: {sql_query}")
    
    # åˆ†å‰²å¤šæ¡SQLè¯­å¥ï¼ˆä»¥åˆ†å·åˆ†éš”ï¼‰
    sql_statements = [stmt.strip() for stmt in sql_query.split(';') if stmt.strip()]
    
    if not sql_statements:
        print(f"âŒ [SQL DEBUG] æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„SQLè¯­å¥")
        return {"db_results": {"query_results": []}}
    
    print(f"ğŸ”¢ [SQL DEBUG] æ£€æµ‹åˆ° {len(sql_statements)} æ¡SQLè¯­å¥")
    
    query_results = []
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        for i, sql_stmt in enumerate(sql_statements, 1):
            print(f"\nğŸ” [SQL DEBUG] æ‰§è¡Œç¬¬ {i} æ¡SQL: {sql_stmt[:100]}...")
            
            try:
                cursor.execute(sql_stmt)
                results = cursor.fetchall()
                columns = [description[0] for description in cursor.description] if cursor.description else []
                
                print(f"âœ… [SQL DEBUG] ç¬¬ {i} æ¡SQLæ‰§è¡ŒæˆåŠŸ")
                print(f"ğŸ“Š [SQL DEBUG] è¿”å›åˆ—æ•°: {len(columns)}")
                print(f"ğŸ“ˆ [SQL DEBUG] è¿”å›è¡Œæ•°: {len(results)}")
                
                if columns:
                    print(f"ğŸ·ï¸ [SQL DEBUG] åˆ—å: {', '.join(columns)}")
                
                # æ„å»ºå½“å‰æŸ¥è¯¢çš„JSONç»“æœ
                current_query_result = {
                    "sql_index": i,
                    "sql_statement": sql_stmt,
                    "columns": columns,
                    "data": []
                }
                
                if results:
                    # å°†æ¯è¡Œç»“æœè½¬æ¢ä¸ºJSONæ ¼å¼
                    for row in results:
                        row_dict = {}
                        for j, value in enumerate(row):
                            if j < len(columns):
                                row_dict[columns[j]] = str(value) if value is not None else None
                        current_query_result["data"].append(row_dict)
                    
                    # æ˜¾ç¤ºæŸ¥è¯¢ç»“æœé¢„è§ˆ
                    if len(results) <= 3:
                        print(f"ğŸ“‹ [SQL DEBUG] ç¬¬ {i} æ¡æŸ¥è¯¢ç»“æœ:")
                        for j, row_dict in enumerate(current_query_result["data"]):
                            print(f"   è¡Œ{j+1}: {json.dumps(row_dict, ensure_ascii=False)}")
                    else:
                        print(f"ğŸ“‹ [SQL DEBUG] ç¬¬ {i} æ¡æŸ¥è¯¢ç»“æœï¼ˆå‰3è¡Œï¼‰:")
                        for j, row_dict in enumerate(current_query_result["data"][:3]):
                            print(f"   è¡Œ{j+1}: {json.dumps(row_dict, ensure_ascii=False)}")
                else:
                    print(f"ğŸ“‹ [SQL DEBUG] ç¬¬ {i} æ¡SQLæ— æŸ¥è¯¢ç»“æœ")
                
                # æ·»åŠ åˆ°æ€»ç»“æœä¸­
                query_results.append(current_query_result)
                    
            except Exception as e:
                print(f"âŒ [SQL DEBUG] ç¬¬ {i} æ¡SQLæ‰§è¡Œå¤±è´¥: {str(e)}")
                # å³ä½¿å¤±è´¥ä¹Ÿæ·»åŠ é”™è¯¯ä¿¡æ¯åˆ°ç»“æœä¸­
                error_result = {
                    "sql_index": i,
                    "sql_statement": sql_stmt,
                    "error": str(e),
                    "columns": [],
                    "data": []
                }
                query_results.append(error_result)
                continue
        
        conn.close()
        
        print(f"\nğŸ¯ [SQL DEBUG] æ‰€æœ‰SQLæ‰§è¡Œå®Œæˆ")
        print(f"ğŸ“Š [SQL DEBUG] æ€»æŸ¥è¯¢æ•°: {len(query_results)}")
        
        # ç»Ÿè®¡æ€»ç»“æœæ•°
        total_data_count = sum(len(result["data"]) for result in query_results)
        print(f"ğŸ“ˆ [SQL DEBUG] æ€»æ•°æ®è¡Œæ•°: {total_data_count}")
        
        # è¾“å‡ºæœ€ç»ˆçš„JSONæ ¼å¼ç»“æœ
        final_result = {"db_results": query_results}
        print(f"\nğŸ“‹ [SQL DEBUG] æœ€ç»ˆJSONç»“æœ:")
        print(json.dumps(final_result, ensure_ascii=False, indent=2))
        
        return final_result
        
    except Exception as e:
        print(f"âŒ [SQL DEBUG] æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        return {"db_results": {"error": str(e), "query_results": []}}

async def generate_answer(state: GraphState):
    """æ ¹æ®æŸ¥è¯¢ç»“æœç”Ÿæˆæœ€ç»ˆçš„è‡ªç„¶è¯­è¨€ç­”æ¡ˆ
    
    å¤„ç†æ–°çš„JSONæ ¼å¼æŸ¥è¯¢ç»“æœ
    """
    import json
    
    query = state['query']
    db_results = state['db_results']
    llm = model_manager.get_llm()
    
    print(f"\nğŸ“‹ [DEBUG] å¼€å§‹ç”Ÿæˆç­”æ¡ˆ")
    print(f"ğŸ” [DEBUG] æŸ¥è¯¢é—®é¢˜: {query}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æŸ¥è¯¢ç»“æœ
    is_empty = all(not res['data'] for res in db_results)
    if is_empty:
        final_answer = "æŠ±æ­‰ï¼ŒæŸ¥è¯¢æ‰§è¡Œå¤±è´¥ï¼Œæ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
        print(f"âŒ [DEBUG] æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›é”™è¯¯ç­”æ¡ˆ") 
        
    else:
        answer_prompt = f"""æ ¹æ®ä»¥ä¸‹æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚                             

ç”¨æˆ·é—®é¢˜ï¼š{query}

æŸ¥è¯¢ç»“æœ{db_results}

è¯¥æŸ¥è¯¢ç»“æœæ˜¯ä¸€ä¸ªä¸¥æ ¼jsonæ–‡æ¡£ï¼Œé”®"data"å¯¹åº”çš„å€¼ä¸ºå‡†ç¡®ç­”æ¡ˆï¼Œéƒ¨åˆ†é”®"data"ä¸­çš„å€¼ä¸ºç©ºï¼Œä¸ç”¨ç†ä¼šã€‚
è¯·æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œç”¨è‡ªç„¶è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæœ‰å¤šä¸ªæŸ¥è¯¢ç»“æœï¼Œè¯·ç»¼åˆæ‰€æœ‰ç»“æœè¿›è¡Œå›ç­”ï¼š"""
            
        print(f"ğŸ¤– [DEBUG] è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ")
        messages = [HumanMessage(content=answer_prompt)]
        response = await llm.ainvoke(messages)
        final_answer = str(response.content)            
        print(f"âœ… [DEBUG] ç­”æ¡ˆç”Ÿæˆå®Œæˆfinal_answer: {final_answer[:100]}...")    
    return {"response": final_answer}

# æ„å»ºLangGraph
builder = StateGraph(GraphState)

builder.add_node("get_relevant", get_relevant_sheets)
builder.add_node("rerank", rerank_sheets)
builder.add_node("generate_sql", generate_sql)
builder.add_node("execute_sql", execute_sql)
builder.add_node("generate_answer", generate_answer)

builder.set_entry_point("get_relevant")

builder.add_edge("get_relevant", "rerank")
builder.add_edge("rerank", "generate_sql")
builder.add_edge("generate_sql", "execute_sql")
builder.add_edge("execute_sql", "generate_answer")
builder.add_edge("generate_answer", END)

graph = builder.compile()

async def run_flow(query: str, excel_path: str, db_path: str):
    """ä¼˜åŒ–çš„ä¸»æµç¨‹"""
    print(f"\nğŸš€ [DEBUG] å¼€å§‹å¤„ç†æŸ¥è¯¢æµç¨‹")
    print(f"ğŸ“ [DEBUG] æŸ¥è¯¢å†…å®¹: {query}")
    print(f"ğŸ“ [DEBUG] Excelæ–‡ä»¶: {excel_path}")
    print(f"ğŸ—„ï¸ [DEBUG] æ•°æ®åº“è·¯å¾„: {db_path}")
    print(f"ğŸ”§ [DEBUG] run_flowå‡½æ•°å·²å¯åŠ¨ï¼Œç‰ˆæœ¬: 2024-01-15")
    
    # 1. ä½¿ç”¨ç¼“å­˜çš„Excelåˆ°SQLiteè½¬æ¢
    print(f"\nâš¡ [DEBUG] æ­¥éª¤1: Excelåˆ°SQLiteè½¬æ¢")
    db_path, table_mapping = excel_to_sqlite(excel_path, db_path)
    print(f"ğŸ“Š [DEBUG] è¡¨æ˜ å°„: {table_mapping}")
    print(f"ğŸ’¾ [DEBUG] ç¼“å­˜æ•°æ®åº“è·¯å¾„: {db_path}")

    # 2. æ‡’åŠ è½½å‘é‡æ•°æ®åº“
    print(f"\nğŸ§  [DEBUG] æ­¥éª¤2: åŠ è½½æ¨¡å‹å’Œå‘é‡æ•°æ®åº“")
    llm = model_manager.get_llm()
    embedding_model = model_manager.get_embedding_model()
    vectorstore = await create_and_store_vectors(EXCEL_DIR, llm, embedding_model)
    print(f"âœ… [DEBUG] æ¨¡å‹å’Œå‘é‡æ•°æ®åº“åŠ è½½å®Œæˆ")

    # 3. è¿è¡ŒLangGraph
    print(f"\nğŸ”„ [DEBUG] æ­¥éª¤3: æ‰§è¡ŒLangGraphå·¥ä½œæµ")
    inputs = {
        "query": query,
        "excel_path": excel_path,
        "db_path": db_path,
        "table_mapping": table_mapping,
        "vectorstore": vectorstore,

    }
    result = await graph.ainvoke(inputs)
    print(f"ğŸ¯ [DEBUG] LangGraphæ‰§è¡Œå®Œæˆ")
    
    # 4. æ„å»ºMCPå“åº”
    print(f"\nğŸ“‹ [DEBUG] æ­¥éª¤4: æ„å»ºMCPå“åº”")
    db_results = result.get('db_results', {'db_results': []})
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æŸ¥è¯¢ç»“æœæ•°æ®
    is_empty = all(not res['data'] for res in db_results)
    
    # è·å–LangGraphç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆ
    final_answer = result.get("response", '')
    print(f"ğŸ” [DEBUG] LangGraphè¿”å›çš„response: {final_answer}")
    
    # å¦‚æœLangGraphæ²¡æœ‰ç”Ÿæˆç­”æ¡ˆï¼Œåˆ™æ ¹æ®æ•°æ®æƒ…å†µç”Ÿæˆé»˜è®¤ç­”æ¡ˆ
    if not final_answer:
        if not is_empty:
            final_answer = "æŸ¥è¯¢æˆåŠŸï¼Œå·²æ‰¾åˆ°ç›¸å…³æ•°æ®"
        else:
            final_answer = "æœªæ‰¾åˆ°ç›¸å…³æ•°æ®"
        print(f"ğŸ”„ [DEBUG] ä½¿ç”¨é»˜è®¤ç­”æ¡ˆ: {final_answer}")
    else:
        print(f"âœ… [DEBUG] ä½¿ç”¨LangGraphç”Ÿæˆçš„ç­”æ¡ˆ: {final_answer[:100]}...")
    
    print(f"ğŸ’¬ [DEBUG] æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
    
    mcp_response = {
        "query": result.get('query', ''),
        "answer": final_answer
    }
    
    print(f"âœ… [DEBUG] MCPå“åº”æ„å»ºå®Œæˆ")
    
    return mcp_response

def format_mcp_output(mcp_response: dict) -> str:
    """æ ¼å¼åŒ–MCPè¾“å‡ºä¸ºå‹å¥½çš„JSONå­—ç¬¦ä¸²"""
    return json.dumps(mcp_response, ensure_ascii=False, indent=2)

def main(query: str = None):
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒä¼ å…¥æŸ¥è¯¢å‚æ•°"""
    db_file = "database.db"
    
    # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨å¹¶æ£€æŸ¥æ‰€æœ‰Excelæ–‡ä»¶
    db_manager = get_database_manager()
    db_manager.check_all_files(EXCEL_DIR)
    
    # åˆå§‹åŒ–åˆ—åæ˜ å°„ç”Ÿæˆå™¨å¹¶æ£€æŸ¥æ˜ å°„é…ç½®
    from column_mapping_generator import get_column_mapping_generator
    try:
        print("ğŸ”§ åˆå§‹åŒ–åˆ—åæ˜ å°„ç”Ÿæˆå™¨...")
        column_mapping_generator = get_column_mapping_generator()
        print("âœ… åˆ—åæ˜ å°„ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ åˆ—åæ˜ å°„ç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œï¼Œä½†å¯èƒ½å½±å“æŸ¥è¯¢å‡†ç¡®æ€§")
    
    excel_files = [os.path.join(EXCEL_DIR, f) for f in os.listdir(EXCEL_DIR) if f.endswith(('.xlsx', '.xls'))]
    if not excel_files:
        print(f"{EXCEL_DIR} ç›®å½•ä¸‹æœªæ‰¾åˆ° Excel æ–‡ä»¶ï¼Œè¯·å…ˆä¸Šä¼ ï¼")
        return None
    
    if not query:
        query = "å®šåˆ¶LEDæ™¯è§‚ç¯01çš„å·¥ç¨‹é‡æ˜¯å¤šå°‘ï¼Ÿæ€»ä»·æ˜¯å¤šå°‘ï¼Ÿ"
    
    excel_file = excel_files[0]
    mcp_response = asyncio.run(run_flow(query, excel_file, db_file))
    
    return mcp_response

# æœåŠ¡é¢„çƒ­å‡½æ•°
def warm_up_service():
    """é¢„çƒ­æœåŠ¡ï¼Œæå‰åŠ è½½æ¨¡å‹"""
    try:
        # é¢„åŠ è½½æ¨¡å‹
        model_manager.get_embedding_model()
        print("âœ… åµŒå…¥æ¨¡å‹é¢„çƒ­å®Œæˆ")
        
        model_manager.get_reranker()
        print("âœ… é‡æ’åºæ¨¡å‹é¢„çƒ­å®Œæˆ")
        
        # LLMé‡‡ç”¨æ‡’åŠ è½½ï¼Œåœ¨é¦–æ¬¡æŸ¥è¯¢æ—¶åŠ è½½
        print("âœ… æœåŠ¡é¢„çƒ­å®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸ æœåŠ¡é¢„çƒ­å¤±è´¥: {e}")

if __name__ == "__main__":
    # æœåŠ¡å¯åŠ¨æ—¶é¢„çƒ­
    warm_up_service()
    
    # å½“ç›´æ¥è¿è¡Œæ—¶ï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢
    result = main()
    if result:
        print("\n" + "="*50)
        print("ğŸ“¤ æœ€ç»ˆMCPå“åº”:")
        print("="*50)
        print(format_mcp_output(result))